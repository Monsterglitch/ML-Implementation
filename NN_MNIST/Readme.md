I've implemented a Neural network from scratch especially for the MNIST dataset.
One input layer, two hidden layers, and an output layer make up this neural network.
ReLU serves as the activation function for both hidden layers, and softmax is employed in the output layer.
